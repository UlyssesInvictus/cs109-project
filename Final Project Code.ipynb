{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 100 most representative names for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get our initial data\n",
    "immig = pd.read_csv('Datasets/ImmigrationByRegion.csv').transpose()\n",
    "immig_regions = immig.iloc[0]\n",
    "immig = immig[1:]\n",
    "immig = immig.rename(columns = immig_regions)\n",
    "immig = immig[::-1]\n",
    "display(immig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then clean our names\n",
    "names = pd.read_csv('Datasets/NationalNames.csv', delimiter = ',', usecols = [1, 2, 3, 4])\n",
    "names = names.groupby(['Name', 'Year'])['Count'].sum()\n",
    "names = names.unstack(level=0)\n",
    "display(names.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now groupby 10 and at last we have what we want\n",
    "names_10 = names.groupby([(y / 10) * 10 for y in names.index.values]).sum().fillna(0)\n",
    "display(names_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some useful tools out of the way now\n",
    "# to check for empty values in immigration\n",
    "import re\n",
    "numeric = re.compile(\"\\d+(?:,\\d+)?\")\n",
    "# display(s[s.str.match(\"\\d+(?:,\\d+)?\")])\n",
    "# to normalize series\n",
    "def normalize(series):\n",
    "    max_val = series.max()\n",
    "    min_val = series.min()\n",
    "    return (series - min_val) / (max_val - min_val)\n",
    "def standardize(series):\n",
    "    return (series - series.mean()) / series.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_dict = {}\n",
    "# short circuit example:\n",
    "# for region in immig.columns[:5]:\n",
    "for region in immig.columns:\n",
    "    # this takes a while, so just to track progress--\n",
    "    print 'Calculating ', region, ' :',\n",
    "    region_vals = immig[region].fillna('(NA)')\n",
    "    # filter for only immigration years with present values\n",
    "    #region_vals = region_vals[region_vals.str.match(\"\\d+(?:,\\d+)?\")]\n",
    "    # find the years both names and immigration have\n",
    "    # handle string nature of immigration years\n",
    "    common_years = names_10.index.intersection(region_vals.index.map(int))\n",
    "    # filter immigration years by common years, handling int/str switch\n",
    "    region_vals = region_vals[map(str, common_years)]\n",
    "    # then map back to int, handling commas\n",
    "    region_vals = region_vals.apply(lambda x: int(x.replace(',', '')))\n",
    "    # and normalize\n",
    "    region_vals = normalize(region_vals)\n",
    "    region_dict[region] = []\n",
    "    name_num = 0\n",
    "    # short circuit example:\n",
    "    # for name in names_10.columns[:10000]:\n",
    "    for name in names_10.columns:\n",
    "        # more progress tracking\n",
    "        if name_num % 5000 == 0:\n",
    "            print name, '...',\n",
    "        name_vals = names_10[name]\n",
    "        name_vals = name_vals[common_years]\n",
    "        name_vals = normalize(name_vals)\n",
    "        # handle some empty arrays because 2010 exclusion\n",
    "        if name_vals.isnull().any():\n",
    "            score = -10\n",
    "        else:\n",
    "            # first value is true--actual immigration data\n",
    "            # second value is predict--name data we're trying to use as model\n",
    "            score = r2_score(region_vals, name_vals)\n",
    "        region_dict[region].append((name, score))\n",
    "        name_num += 1\n",
    "    print 'done!' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for region in region_dict:\n",
    "    name_scores = region_dict[region]\n",
    "    name_scores = sorted(name_scores, key=lambda x: -x[1])\n",
    "    name_scores = [(x[0], round(x[1], 2)) for x in name_scores]\n",
    "    region_dict[region] = name_scores\n",
    "    print region, ': ', name_scores[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save names in csv files\n",
    "You can skip this section since these individual files were later compiled into a single file: \"represnames.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_Total = region_dict['Total'][:100]\n",
    "names_Total = []\n",
    "for i in range(100):\n",
    "    name = scores_Total[i][0]\n",
    "    names_Total.append(name)\n",
    "\n",
    "Total_csv = pd.DataFrame({'Names': names_Total})\n",
    "Total_csv.to_csv('names_Total.csv', sep=',', columns = ['Names'], header = ['Names'],\n",
    "                         index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_Europe = region_dict['Europe'][:100]\n",
    "names_Europe = []\n",
    "for i in range(100):\n",
    "    name = scores_Europe[i][0]\n",
    "    names_Europe.append(name)\n",
    "\n",
    "Europe_csv = pd.DataFrame({'Names': names_Europe})\n",
    "Europe_csv.to_csv('names_Europe.csv', sep=',', columns = ['Names'], header = ['Names'],\n",
    "                         index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_Oceania = region_dict['Oceania'][:100]\n",
    "names_Oceania = []\n",
    "for i in range(100):\n",
    "    name = scores_Oceania[i][0]\n",
    "    names_Oceania.append(name)\n",
    "\n",
    "Oceania_csv = pd.DataFrame({'Names': names_Oceania})\n",
    "Oceania_csv.to_csv('names_Oceania.csv', sep=',', columns = ['Names'], header = ['Names'],\n",
    "                         index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_Africa = region_dict['Africa'][:100]\n",
    "names_Africa = []\n",
    "for i in range(100):\n",
    "    name = scores_Africa[i][0]\n",
    "    names_Africa.append(name)\n",
    "\n",
    "Africa_csv = pd.DataFrame({'Names': names_Africa})\n",
    "Africa_csv.to_csv('names_Africa.csv', sep=',', columns = ['Names'], header = ['Names'],\n",
    "                         index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_Asia = region_dict['Asia'][:100]\n",
    "names_Asia = []\n",
    "for i in range(100):\n",
    "    name = scores_Asia[i][0]\n",
    "    names_Asia.append(name)\n",
    "\n",
    "Asia_csv = pd.DataFrame({'Names': names_Asia})\n",
    "Asia_csv.to_csv('names_Asia.csv', sep=',', columns = ['Names'], header = ['Names'],\n",
    "                         index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_Americas = region_dict['Americas'][:100]\n",
    "names_Americas = []\n",
    "for i in range(100):\n",
    "    name = scores_Americas[i][0]\n",
    "    names_Americas.append(name)\n",
    "\n",
    "Americas_csv = pd.DataFrame({'Names': names_Americas})\n",
    "Americas_csv.to_csv('names_Americas.csv', sep=',', columns = ['Names'], header = ['Names'],\n",
    "                         index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most representative names by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get our state names again\n",
    "state_names = pd.read_csv('Datasets/StateNames.csv', delimiter = ',', usecols = [1, 2, 3, 4, 5])\n",
    "# slow, but less annoying than typing in manually...\n",
    "state_list = np.unique(state_names['State'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get our most representative names\n",
    "region_names = pd.read_csv('Datasets/repres_names/repres_names.csv', delimiter = ',')\n",
    "display(region_names.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# requires a state_list and region_names global\n",
    "def cross_best_names(region, year, byDecade=False):\n",
    "    # build a dictionary of counts by state and then name\n",
    "    # count by decade as an option\n",
    "    years = range(year, year + 11 if byDecade else year + 1)\n",
    "    state_names_by_year = state_names[state_names['Year'].isin(years)]\n",
    "    names_dict = {}\n",
    "    names = state_names_by_year.iloc[:,0].values\n",
    "    counts = state_names_by_year.iloc[:,-1].values\n",
    "    states = state_names_by_year.iloc[:,-2].values\n",
    "    for i in range(len(state_names_by_year.index)):\n",
    "        if states[i] not in names_dict:\n",
    "            names_dict[states[i]] = {}\n",
    "        if names[i] not in names_dict[states[i]]:\n",
    "            names_dict[states[i]][names[i]] = counts[i]\n",
    "        else:\n",
    "            names_dict[states[i]][names[i]] += counts[i]\n",
    "    # build our matrix of names by states\n",
    "    df = pd.DataFrame(index=state_list)\n",
    "    for n in region_names[region]:\n",
    "        state_name_counts = []\n",
    "        for s in state_list:\n",
    "            if s in names_dict and n in names_dict[s]:\n",
    "                state_name_counts.append(names_dict[s][n])\n",
    "            else:\n",
    "                state_name_counts.append(0)    \n",
    "        df[n] = pd.Series(state_name_counts, index=state_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regions = pd.read_csv('Datasets/regions.csv', delimiter = ',', usecols = [2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to plot first two principal components\n",
    "def pca_per_decaderegion(region, decade, ax):\n",
    "    x = cross_best_names(region, decade, byDecade=True)\n",
    "    df_norm = (x-x.mean())/x.std()\n",
    "    df_norm = df_norm.fillna(0)\n",
    "    #Apply PCA to data and get the top 2 axes of maximum variation\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(df_norm.values)\n",
    "\n",
    "    #Project to the data onto the two axes\n",
    "    x_reduced = pca.transform(df_norm.values)\n",
    "    \n",
    "    region_x = pd.DataFrame(x_reduced, columns=['PC-1', 'PC-2'])\n",
    "    us_region = region_x.join(regions)\n",
    "    \n",
    "    #print x_reduced\n",
    "    #Visualized our reduced data\n",
    "    #region.ix[region['region']== 'South', 1]\n",
    "    ax.scatter(us_region.ix[us_region.iloc[:,2] == 'South', 0], us_region.ix[us_region.iloc[:,2] == 'South', 1], \n",
    "           color='b', label = 'South')\n",
    "    ax.scatter(us_region.ix[us_region.iloc[:,2] == 'West', 0], us_region.ix[us_region.iloc[:,2] == 'West', 1], \n",
    "           color='g', label = 'West')\n",
    "    ax.scatter(us_region.ix[us_region.iloc[:,2] == 'Midwest', 0], us_region.ix[us_region.iloc[:,2] == 'Midwest', 1], \n",
    "           color='r', label = 'Midwest')\n",
    "    ax.scatter(us_region.ix[us_region.iloc[:,2] == 'Northeast', 0], us_region.ix[us_region.iloc[:,2] == 'Northeast', 1], \n",
    "           color='y', label = 'Northeast')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_title(str(region) + str(decade)+'data projected onto the first 2 PCA components')\n",
    "    ax.legend()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting first two principal components for Europe per decade\n",
    "fig, ax = plt.subplots(5, 2, figsize = (20,25))\n",
    "ax[0,0] = pca_per_decaderegion('Europe',1910, ax[0,0])\n",
    "ax[0,1] = pca_per_decaderegion('Europe',1920, ax[0,1])\n",
    "ax[1,0] = pca_per_decaderegion('Europe',1930, ax[1,0])\n",
    "ax[1,1] = pca_per_decaderegion('Europe',1940, ax[1,1])\n",
    "ax[2,0] = pca_per_decaderegion('Europe',1950, ax[2,0])\n",
    "ax[2,1] = pca_per_decaderegion('Europe',1960, ax[2,1])\n",
    "ax[3,0] = pca_per_decaderegion('Europe',1970, ax[3,0])\n",
    "ax[3,1] = pca_per_decaderegion('Europe',1980, ax[3,1])\n",
    "ax[4,0] = pca_per_decaderegion('Europe',1990, ax[4,0])\n",
    "ax[4,1] = pca_per_decaderegion('Europe',2000, ax[4,1])\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.savefig('pca_europe.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names with most variance in principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to print names that account for most variance in each principal component\n",
    "def babyname_variance_explained(region, decade):\n",
    "    x = cross_best_names(region, decade, byDecade=True)\n",
    "    df_norm = (x-x.mean())/x.std()\n",
    "    df_norm = df_norm.fillna(0)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(df_norm.values)\n",
    "    #Project to the data onto the two axes\n",
    "    x_reduced = pca.transform(df_norm.values)\n",
    "    \n",
    "    identify = pd.DataFrame(pca.components_.T, columns=['PC-1', 'PC-2'], index=x.columns)\n",
    "\n",
    "    identify_abs = identify.abs()\n",
    "    \n",
    "    print \"Most represented names in\", region , str(decade) + ':' , identify_abs['PC-1'].idxmax(),identify_abs['PC-2'].idxmax()\n",
    "    identify_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most unique name by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_name_bystate(region, decade):\n",
    "    x = cross_best_names(region, decade, byDecade=True)\n",
    "    df_norm = (x-x.mean())/x.std()\n",
    "    df_norm = df_norm.fillna(0)\n",
    "    df_norm.head()\n",
    "\n",
    "    return df_norm.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eu_names_1910 = unique_name_bystate('Europe',1910)\n",
    "eu_names_1920 = unique_name_bystate('Europe',1920)\n",
    "eu_names_1930 = unique_name_bystate('Europe',1930)\n",
    "eu_names_1940 = unique_name_bystate('Europe',1940)\n",
    "eu_names_1950 = unique_name_bystate('Europe',1950)\n",
    "eu_names_1960 = unique_name_bystate('Europe',1960)\n",
    "eu_names_1970 = unique_name_bystate('Europe',1970)\n",
    "eu_names_1980 = unique_name_bystate('Europe',1980)\n",
    "eu_names_1990 = unique_name_bystate('Europe',1990)\n",
    "eu_names_2000 = unique_name_bystate('Europe',2000)\n",
    "eu_names_2010 = unique_name_bystate('Europe',2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states_list = list(eu_names_1910.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make into a dataframe\n",
    "d = {1910 : pd.Series(eu_names_1910.values, index = states_list),\n",
    "    1920 : pd.Series(eu_names_1920.values, index = states_list),\n",
    "    1930 : pd.Series(eu_names_1930.values, index = states_list),\n",
    "    1940 : pd.Series(eu_names_1940.values, index = states_list),\n",
    "    1950 : pd.Series(eu_names_1950.values, index = states_list),\n",
    "    1960 : pd.Series(eu_names_1960.values, index = states_list),\n",
    "    1970 : pd.Series(eu_names_1970.values, index = states_list),\n",
    "    1980 : pd.Series(eu_names_1980.values, index = states_list),\n",
    "    1990 : pd.Series(eu_names_1990.values, index = states_list),\n",
    "    2000 : pd.Series(eu_names_2000.values, index = states_list),\n",
    "    2010 : pd.Series(eu_names_2010.values, index = states_list),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d, index=states_list, columns=decades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('Datasets/unique_names_bystate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region = 'Europe'\n",
    "decade = 1920\n",
    "x = cross_best_names(region, decade, byDecade=True)\n",
    "df_norm = (x-x.mean())/x.std()\n",
    "df_norm = df_norm.fillna(0)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df_norm.values)\n",
    "x_reduced = pca.transform(df_norm.values)\n",
    "region_x = pd.DataFrame(x_reduced, columns=['PC-1', 'PC-2'])\n",
    "us_region = region_x.join(regions)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans = kmeans.fit(us_region.iloc[:,[0,1]])\n",
    "# make a copy, just so we don't mess with the previous for testing\n",
    "classes = us_region.copy()\n",
    "classes['guess'] = pd.Series(kmeans.labels_, index=us_region.index)\n",
    "display(classes)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
